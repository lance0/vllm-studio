"""FastAPI application - minimal controller API."""

from __future__ import annotations

import asyncio
import datetime as dt
import json
from collections import deque
from pathlib import Path
from typing import Optional

import httpx
from fastapi import Depends, FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse

from . import __version__
from .events import event_manager, Event
from .metrics import (
    update_active_model, update_gpu_metrics, update_sse_metrics,
    get_metrics_content, get_metrics_content_type
)
from .config import settings
from .gpu import get_gpu_info
from .models import HealthResponse, LaunchResult, OpenAIModelInfo, OpenAIModelList, Recipe
from .process import evict_model, find_inference_process, switch_model
from .store import RecipeStore

app = FastAPI(
    title="vLLM Studio Controller",
    version=__version__,
    description="Minimal model lifecycle management for vLLM/SGLang",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global state
_store: Optional[RecipeStore] = None
_switch_lock = asyncio.Lock()
_broadcast_task: Optional[asyncio.Task] = None

import logging
import time

logger = logging.getLogger(__name__)
access_logger = logging.getLogger("vllm_studio.access")

# Configure access logger
if not access_logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter(
        '%(asctime)s ACCESS %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    access_logger.addHandler(handler)
    access_logger.setLevel(logging.INFO)


def get_store() -> RecipeStore:
    global _store
    if _store is None:
        _store = RecipeStore(settings.db_path)
    return _store


# --- Access logging & authentication middleware ---
@app.middleware("http")
async def access_logging_middleware(request: Request, call_next):
    """Log all requests with detailed info for security monitoring."""
    start_time = time.time()

    # Extract request info
    client_ip = request.headers.get("CF-Connecting-IP") or \
                request.headers.get("X-Forwarded-For", "").split(",")[0].strip() or \
                request.client.host if request.client else "unknown"
    method = request.method
    path = request.url.path
    query = str(request.url.query) if request.url.query else ""
    user_agent = request.headers.get("User-Agent", "unknown")[:100]
    referer = request.headers.get("Referer", "-")[:200]
    auth_header = request.headers.get("Authorization", "")
    has_auth = bool(auth_header)
    auth_valid = False

    # Check auth status
    public_paths = {"/health", "/docs", "/openapi.json", "/redoc", "/metrics"}
    is_public = path in public_paths

    if settings.api_key:
        if auth_header.startswith("Bearer "):
            auth_valid = auth_header.split(" ", 1)[1] == settings.api_key
    else:
        auth_valid = True  # No API key configured

    # Determine if request should be blocked
    blocked = False
    if settings.api_key and not is_public and not auth_valid:
        blocked = True

    # Process request
    if blocked:
        response = JSONResponse(status_code=401, content={"error": "Invalid or missing API key"})
        status_code = 401
    else:
        response = await call_next(request)
        status_code = response.status_code

    # Calculate duration
    duration_ms = (time.time() - start_time) * 1000

    # Log with security-relevant details
    log_parts = [
        f"ip={client_ip}",
        f"method={method}",
        f"path={path}",
        f"query={query}" if query else None,
        f"status={status_code}",
        f"duration={duration_ms:.1f}ms",
        f"auth={'valid' if auth_valid else ('invalid' if has_auth else 'none')}",
        f"blocked={blocked}",
        f"ua={user_agent}",
        f"referer={referer}" if referer != "-" else None,
    ]
    log_msg = " | ".join(filter(None, log_parts))

    # Log level based on status
    if blocked or status_code >= 400:
        access_logger.warning(log_msg)
    else:
        access_logger.info(log_msg)

    return response


# --- Real-time events (SSE) startup/shutdown ---
@app.on_event("startup")
async def startup_event():
    """Start background tasks on application startup."""
    global _broadcast_task
    _broadcast_task = asyncio.create_task(broadcast_updates())
    logger.info("Started background SSE broadcast task")


@app.on_event("shutdown")
async def shutdown_event():
    """Clean up background tasks on shutdown."""
    global _broadcast_task
    if _broadcast_task:
        _broadcast_task.cancel()
        try:
            await _broadcast_task
        except asyncio.CancelledError:
            pass
    logger.info("Stopped background SSE broadcast task")


async def broadcast_updates():
    """Background task to broadcast status/GPU/metrics updates every second."""
    while True:
        try:
            # Broadcast process status
            current = find_inference_process(settings.inference_port)
            status_data = {
                "running": current is not None,
                "process": current.model_dump() if current else None,
                "inference_port": settings.inference_port,
            }
            await event_manager.publish_status(status_data)

            # Broadcast GPU metrics
            gpu_list = get_gpu_info()
            await event_manager.publish_gpu([gpu.model_dump() for gpu in gpu_list])

            # Broadcast vLLM metrics (if backend is running)
            if current:
                try:
                    async with httpx.AsyncClient(timeout=2) as client:
                        r = await client.get(f"http://localhost:{settings.inference_port}/metrics")
                        if r.status_code == 200:
                            metrics = parse_vllm_metrics(r.text)
                            await event_manager.publish_metrics(metrics)
                except Exception:
                    pass  # Backend not ready or metrics not available

        except Exception as e:
            logger.error(f"Error in broadcast_updates: {e}")

        await asyncio.sleep(1)  # Update every 1 second


def parse_vllm_metrics(prometheus_text: str) -> dict:
    """Parse Prometheus format metrics from vLLM.

    Extracts key metrics like:
    - vllm:num_requests_running
    - vllm:num_requests_waiting
    - vllm:avg_generation_throughput_toks_per_s
    - vllm:gpu_cache_usage_perc
    """
    metrics = {}

    for line in prometheus_text.split('\n'):
        if line.startswith('#') or not line.strip():
            continue

        try:
            parts = line.split()
            if len(parts) >= 2:
                metric_name = parts[0]
                metric_value = float(parts[1])

                # Map vLLM metrics to friendly names
                if 'num_requests_running' in metric_name:
                    metrics['running_requests'] = int(metric_value)
                elif 'num_requests_waiting' in metric_name:
                    metrics['pending_requests'] = int(metric_value)
                elif 'avg_generation_throughput' in metric_name:
                    metrics['generation_throughput'] = metric_value
                elif 'avg_prompt_throughput' in metric_name:
                    metrics['prompt_throughput'] = metric_value
                elif 'gpu_cache_usage_perc' in metric_name:
                    metrics['kv_cache_usage'] = metric_value / 100
                elif 'time_to_first_token' in metric_name:
                    if 'sum' in metric_name:
                        metrics['ttft_sum'] = metric_value
                    elif 'count' in metric_name:
                        metrics['ttft_count'] = int(metric_value)
        except Exception:
            continue

    # Calculate average TTFT if available
    if 'ttft_sum' in metrics and 'ttft_count' in metrics and metrics['ttft_count'] > 0:
        metrics['avg_ttft_ms'] = (metrics['ttft_sum'] / metrics['ttft_count']) * 1000

    return metrics


# --- Health ---
@app.get("/health", response_model=HealthResponse, tags=["System"])
async def health():
    """Health check."""
    current = find_inference_process(settings.inference_port)
    inference_ready = False

    if current:
        try:
            async with httpx.AsyncClient(timeout=5) as client:
                r = await client.get(f"http://localhost:{settings.inference_port}/health")
                inference_ready = r.status_code == 200
        except Exception:
            pass

    return HealthResponse(
        status="ok",
        version=__version__,
        inference_ready=inference_ready,
        backend_reachable=inference_ready,
        running_model=current.served_model_name or current.model_path if current else None,
    )


@app.get("/status", tags=["System"])
async def status():
    """Detailed status."""
    current = find_inference_process(settings.inference_port)
    return {
        "running": current is not None,
        "process": current.model_dump() if current else None,
        "inference_port": settings.inference_port,
    }


@app.get("/gpus", tags=["System"])
async def gpus():
    """Get GPU information."""
    gpu_list = get_gpu_info()
    return {
        "count": len(gpu_list),
        "gpus": [gpu.model_dump() for gpu in gpu_list],
    }


# --- OpenAI-compatible endpoints ---
@app.get("/v1/models", response_model=OpenAIModelList, tags=["OpenAI Compatible"])
async def list_models_openai(store: RecipeStore = Depends(get_store)):
    """List all models in OpenAI format."""
    import time

    recipes = store.list()
    current = find_inference_process(settings.inference_port)

    # Get active model metadata from vLLM/SGLang if available
    active_model_data = None
    if current:
        try:
            async with httpx.AsyncClient(timeout=5) as client:
                r = await client.get(f"http://localhost:{settings.inference_port}/v1/models")
                if r.status_code == 200:
                    active_model_data = r.json()
        except Exception:
            pass

    models = []
    current_time = int(time.time())

    for recipe in recipes:
        is_active = False
        max_model_len = recipe.max_model_len

        # Check if this recipe is the currently running model
        if current:
            # Match by served_model_name first (most reliable)
            if current.served_model_name and recipe.served_model_name == current.served_model_name:
                is_active = True
            # Or match by model_path
            elif current.model_path:
                if recipe.model_path in current.model_path or current.model_path in recipe.model_path:
                    is_active = True
                elif current.model_path.split("/")[-1] == recipe.model_path.split("/")[-1]:
                    is_active = True
            # Try to get max_model_len from the active model's endpoint
            if active_model_data and "data" in active_model_data:
                for model in active_model_data["data"]:
                    if "max_model_len" in model:
                        max_model_len = model["max_model_len"]
                        break

        # Use served_model_name if available, otherwise use recipe ID
        model_id = recipe.served_model_name or recipe.id

        models.append(
            OpenAIModelInfo(
                id=model_id,
                created=current_time,
                active=is_active,
                max_model_len=max_model_len,
            )
        )

    return OpenAIModelList(data=models)


@app.get("/v1/models/{model_id}", response_model=OpenAIModelInfo, tags=["OpenAI Compatible"])
async def get_model_openai(model_id: str, store: RecipeStore = Depends(get_store)):
    """Get a specific model in OpenAI format."""
    import time

    # Try to find by served_model_name first, then by ID
    recipes = store.list()
    recipe = None
    for r in recipes:
        if (r.served_model_name and r.served_model_name == model_id) or r.id == model_id:
            recipe = r
            break

    if not recipe:
        raise HTTPException(status_code=404, detail="Model not found")

    current = find_inference_process(settings.inference_port)
    is_active = False
    max_model_len = recipe.max_model_len

    # Check if this is the active model and get metadata
    if current and current.model_path and recipe.model_path in current.model_path:
        is_active = True
        try:
            async with httpx.AsyncClient(timeout=5) as client:
                r = await client.get(f"http://localhost:{settings.inference_port}/v1/models")
                if r.status_code == 200:
                    active_model_data = r.json()
                    if "data" in active_model_data:
                        for model in active_model_data["data"]:
                            if "max_model_len" in model:
                                max_model_len = model["max_model_len"]
                                break
        except Exception:
            pass

    # Use served_model_name if available, otherwise use recipe ID
    display_id = recipe.served_model_name or recipe.id

    return OpenAIModelInfo(
        id=display_id,
        created=int(time.time()),
        active=is_active,
        max_model_len=max_model_len,
    )


# --- Recipes ---
@app.get("/recipes", tags=["Recipes"])
async def list_recipes(store: RecipeStore = Depends(get_store)):
    """List all recipes."""
    recipes = store.list()
    current = find_inference_process(settings.inference_port)
    result = []
    for r in recipes:
        status = "stopped"
        if current:
            # Match by served_model_name first (most reliable)
            if current.served_model_name and r.served_model_name == current.served_model_name:
                status = "running"
            # Or match by model_path (check both directions for relative/absolute paths)
            elif current.model_path:
                if r.model_path in current.model_path or current.model_path in r.model_path:
                    status = "running"
                # Also check basename match
                elif current.model_path.split("/")[-1] == r.model_path.split("/")[-1]:
                    status = "running"
        result.append({**r.model_dump(), "status": status})
    return result


@app.get("/recipes/{recipe_id}", tags=["Recipes"])
async def get_recipe(recipe_id: str, store: RecipeStore = Depends(get_store)):
    """Get a recipe by ID."""
    recipe = store.get(recipe_id)
    if not recipe:
        raise HTTPException(status_code=404, detail="Recipe not found")
    return recipe


@app.post("/recipes", tags=["Recipes"])
async def create_recipe(recipe: Recipe, store: RecipeStore = Depends(get_store)):
    """Create or update a recipe."""
    store.save(recipe)
    return {"success": True, "id": recipe.id}


@app.put("/recipes/{recipe_id}", tags=["Recipes"])
async def update_recipe(recipe_id: str, recipe: Recipe, store: RecipeStore = Depends(get_store)):
    """Update a recipe by ID."""
    if recipe.id != recipe_id:
        recipe.id = recipe_id
    store.save(recipe)
    return {"success": True, "id": recipe.id}


@app.delete("/recipes/{recipe_id}", tags=["Recipes"])
async def delete_recipe(recipe_id: str, store: RecipeStore = Depends(get_store)):
    """Delete a recipe."""
    if not store.delete(recipe_id):
        raise HTTPException(status_code=404, detail="Recipe not found")
    return {"success": True}


# --- Model lifecycle ---
@app.post("/launch/{recipe_id}", response_model=LaunchResult, tags=["Lifecycle"])
async def launch(recipe_id: str, force: bool = False, store: RecipeStore = Depends(get_store)):
    """Launch a model by recipe ID with real-time progress updates.

    Progress events are emitted via SSE:
    - evicting: Stopping current model
    - launching: Starting new model
    - waiting: Waiting for model to be ready
    - ready: Model is ready to serve
    - error: Launch failed
    """
    import time

    recipe = store.get(recipe_id)
    if not recipe:
        raise HTTPException(status_code=404, detail="Recipe not found")

    async with _switch_lock:
        # Stage 1: Evict current model
        await event_manager.publish_launch_progress(
            recipe_id, "evicting", "Stopping current model...", progress=0.0
        )
        await evict_model(force=force)
        await asyncio.sleep(2)

        # Stage 2: Launch new model
        await event_manager.publish_launch_progress(
            recipe_id, "launching", f"Starting {recipe.name}...", progress=0.25
        )

        # Import launch_model from process module
        from .process import launch_model
        success, pid, message = await launch_model(recipe)

        if not success:
            await event_manager.publish_launch_progress(
                recipe_id, "error", message, progress=0.0
            )
            return LaunchResult(success=False, pid=None, message=message, log_file=None)

        # Stage 3: Wait for readiness
        await event_manager.publish_launch_progress(
            recipe_id, "waiting", "Waiting for model to load...", progress=0.5
        )

        # Poll health endpoint (up to 5 minutes)
        start = time.time()
        timeout = 300
        ready = False
        crashed = False

        while time.time() - start < timeout:
            # Check if process has crashed
            try:
                import psutil
                if pid and not psutil.pid_exists(pid):
                    crashed = True
                    break
            except Exception:
                pass

            try:
                async with httpx.AsyncClient(timeout=5) as client:
                    r = await client.get(f"http://localhost:{settings.inference_port}/health")
                    if r.status_code == 200:
                        ready = True
                        break
            except Exception:
                pass

            elapsed = int(time.time() - start)
            await event_manager.publish_launch_progress(
                recipe_id, "waiting",
                f"Loading model... ({elapsed}s)",
                progress=0.5 + (elapsed / timeout) * 0.5
            )
            await asyncio.sleep(3)

        if crashed:
            # Read the last lines from the log file for error context
            log_file = Path(f"/tmp/vllm_{recipe_id}.log")
            error_tail = ""
            if log_file.exists():
                try:
                    error_tail = log_file.read_text()[-1000:]
                except Exception:
                    pass
            await event_manager.publish_launch_progress(
                recipe_id, "error", f"Model process crashed. Check logs for details.", progress=0.0
            )
            return LaunchResult(success=False, pid=None, message=f"Process crashed: {error_tail[-200:]}", log_file=str(log_file))

        if ready:
            await event_manager.publish_launch_progress(
                recipe_id, "ready", "Model is ready!", progress=1.0
            )
        else:
            await event_manager.publish_launch_progress(
                recipe_id, "error", "Model failed to become ready (timeout)", progress=0.0
            )

    return LaunchResult(
        success=success,
        pid=pid,
        message=message,
        log_file=f"/tmp/vllm_{recipe_id}.log" if success else None,
    )


@app.post("/evict", tags=["Lifecycle"])
async def evict(force: bool = False):
    """Stop the running model."""
    async with _switch_lock:
        pid = await evict_model(force=force)
    return {"success": True, "evicted_pid": pid}


@app.get("/wait-ready", tags=["Lifecycle"])
async def wait_ready(timeout: int = 300):
    """Wait for inference backend to be ready."""
    import time

    start = time.time()
    while time.time() - start < timeout:
        try:
            async with httpx.AsyncClient(timeout=5) as client:
                r = await client.get(f"http://localhost:{settings.inference_port}/health")
                if r.status_code == 200:
                    return {"ready": True, "elapsed": int(time.time() - start)}
        except Exception:
            pass
        await asyncio.sleep(2)

    return {"ready": False, "elapsed": timeout, "error": "Timeout waiting for backend"}


# --- Logs ---
def _log_path_for(session_id: str) -> Path:
    safe = "".join(ch for ch in (session_id or "") if ch.isalnum() or ch in ("-", "_", "."))
    if not safe:
        raise HTTPException(status_code=400, detail="Invalid log session id")
    return Path("/tmp") / f"vllm_{safe}.log"


def _tail_lines(path: Path, limit: int) -> list[str]:
    try:
        with path.open("r", encoding="utf-8", errors="replace") as f:
            return list(deque(f, maxlen=limit))
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="Log not found")


@app.get("/logs", tags=["Logs"])
async def list_logs(store: RecipeStore = Depends(get_store)):
    """List available inference log files."""
    current = find_inference_process(settings.inference_port)
    log_files = sorted(Path("/tmp").glob("vllm_*.log"), key=lambda p: p.stat().st_mtime, reverse=True)

    sessions = []
    for p in log_files:
        sid = p.name.removeprefix("vllm_").removesuffix(".log")
        recipe = store.get(sid)
        created_at = dt.datetime.fromtimestamp(p.stat().st_mtime, tz=dt.timezone.utc).isoformat()

        status = "stopped"
        if current and recipe and current.model_path and recipe.model_path and recipe.model_path in current.model_path:
            status = "running"
        elif current and recipe and current.served_model_name and recipe.served_model_name == current.served_model_name:
            status = "running"

        sessions.append(
            {
                "id": sid,
                "recipe_id": recipe.id if recipe else sid,
                "recipe_name": recipe.name if recipe else None,
                "model_path": recipe.model_path if recipe else None,
                "model": (recipe.served_model_name or recipe.name) if recipe else sid,
                "backend": recipe.backend.value if recipe else None,
                "created_at": created_at,
                "status": status,
            }
        )

    return {"sessions": sessions}


@app.get("/logs/{session_id}", tags=["Logs"])
async def get_logs(session_id: str, limit: int = 2000):
    """Get log content for a session (returns both `logs` and `content` for UI compatibility)."""
    limit = max(1, min(int(limit), 20000))
    path = _log_path_for(session_id)
    lines = _tail_lines(path, limit)
    logs = [ln.rstrip("\n") for ln in lines]
    return {"id": session_id, "logs": logs, "content": "\n".join(logs)}


@app.delete("/logs/{session_id}", tags=["Logs"])
async def delete_logs(session_id: str):
    """Delete a log file."""
    path = _log_path_for(session_id)
    try:
        path.unlink()
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="Log not found")
    return {"success": True}


# --- Real-time events (SSE) endpoints ---
@app.get("/events", tags=["Events"])
async def events_stream():
    """Subscribe to real-time status updates via Server-Sent Events.

    Events emitted:
    - status: Process status (running/stopped, model info)
    - gpu: GPU metrics (utilization, memory, temperature)
    - metrics: vLLM performance metrics
    - launch_progress: Model launch progress

    Example usage:
        const es = new EventSource('/events');
        es.addEventListener('status', (e) => console.log(JSON.parse(e.data)));
    """
    async def event_generator():
        try:
            async for event in event_manager.subscribe():
                yield event.to_sse()
        except asyncio.CancelledError:
            pass  # Client disconnected

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-transform",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )


@app.get("/logs/{session_id}/stream", tags=["Logs"])
async def stream_logs(session_id: str):
    """Stream log file updates in real-time via SSE.

    First sends existing log content, then tails new lines as they're added.
    """
    path = _log_path_for(session_id)

    async def log_generator():
        try:
            # Send existing content first
            if path.exists():
                with path.open("r", encoding="utf-8", errors="replace") as f:
                    for line in f:
                        event = Event(type="log", data={"line": line.rstrip("\n")})
                        yield event.to_sse()

            # Then stream new lines
            async for event in event_manager.subscribe(f"logs:{session_id}"):
                yield event.to_sse()
        except asyncio.CancelledError:
            pass

    return StreamingResponse(
        log_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-transform",
            "Connection": "keep-alive",
        }
    )


@app.get("/events/stats", tags=["Events"])
async def events_stats():
    """Get event manager statistics for monitoring."""
    return event_manager.get_stats()


# --- Prometheus metrics ---
@app.get("/metrics", tags=["Monitoring"])
async def metrics():
    """Prometheus metrics endpoint.

    Exposes metrics for:
    - Model switches and launch failures
    - GPU utilization, memory, temperature
    - SSE connection stats
    - Active model information
    """
    from fastapi.responses import Response

    # Update metrics before serving
    current = find_inference_process(settings.inference_port)
    if current:
        update_active_model(
            model_path=current.model_path,
            backend=current.backend,
            served_name=current.served_model_name
        )
    else:
        update_active_model()

    gpu_list = get_gpu_info()
    update_gpu_metrics([gpu.model_dump() for gpu in gpu_list])

    sse_stats = event_manager.get_stats()
    update_sse_metrics(sse_stats)

    return Response(
        content=get_metrics_content(),
        media_type=get_metrics_content_type()
    )


# --- MCP (minimal built-in tools) ---
_MCP_CFG_NAME = "mcp_servers.json"


def _mcp_cfg_path() -> Path:
    settings.data_dir.mkdir(parents=True, exist_ok=True)
    return settings.data_dir / _MCP_CFG_NAME


def _read_mcp_servers() -> list[dict]:
    path = _mcp_cfg_path()
    if not path.exists():
        return []
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        if isinstance(data, list):
            return [s for s in data if isinstance(s, dict)]
    except Exception:
        return []
    return []


def _write_mcp_servers(servers: list[dict]) -> None:
    _mcp_cfg_path().write_text(json.dumps(servers, indent=2, sort_keys=True), encoding="utf-8")


@app.get("/mcp/servers", tags=["MCP"])
async def list_mcp_servers():
    return _read_mcp_servers()


@app.post("/mcp/servers", tags=["MCP"])
async def add_mcp_server(server: dict):
    name = str(server.get("name") or "").strip()
    command = str(server.get("command") or "").strip()
    if not name or not command:
        raise HTTPException(status_code=400, detail="`name` and `command` required")

    servers = [s for s in _read_mcp_servers() if s.get("name") != name]
    server["name"] = name
    server["command"] = command
    server["enabled"] = bool(server.get("enabled", True))
    server["args"] = list(server.get("args") or [])
    server["env"] = dict(server.get("env") or {})
    servers.append(server)
    _write_mcp_servers(servers)
    return {"success": True}


@app.put("/mcp/servers/{name}", tags=["MCP"])
async def update_mcp_server(name: str, server: dict):
    server["name"] = name
    return await add_mcp_server(server)


@app.delete("/mcp/servers/{name}", tags=["MCP"])
async def delete_mcp_server(name: str):
    servers = _read_mcp_servers()
    next_servers = [s for s in servers if s.get("name") != name]
    if len(next_servers) == len(servers):
        raise HTTPException(status_code=404, detail="Server not found")
    _write_mcp_servers(next_servers)
    return {"success": True}


@app.get("/mcp/tools", tags=["MCP"])
async def list_mcp_tools():
    # Provide a small built-in tool set so tool-calling works out of the box.
    return {
        "tools": [
            {
                "server": "builtin",
                "name": "time",
                "description": "Get the current time (UTC) as an ISO 8601 string.",
                "input_schema": {"type": "object", "properties": {}, "additionalProperties": False},
            },
            {
                "server": "builtin",
                "name": "fetch",
                "description": "Fetch a URL via HTTP GET and return text (truncated).",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string", "description": "URL to fetch"},
                        "max_bytes": {"type": "integer", "description": "Max bytes to return", "default": 20000},
                        "timeout_sec": {"type": "number", "description": "Request timeout seconds", "default": 20},
                        "headers": {"type": "object", "additionalProperties": {"type": "string"}},
                    },
                    "required": ["url"],
                    "additionalProperties": False,
                },
            },
        ]
    }


@app.post("/mcp/tools/{server}/{tool_name}", tags=["MCP"])
async def call_mcp_tool(server: str, tool_name: str, payload: dict):
    if server != "builtin":
        raise HTTPException(status_code=404, detail="Unknown MCP server")

    if tool_name == "time":
        return {"result": {"utc": dt.datetime.now(tz=dt.timezone.utc).isoformat()}}

    if tool_name == "fetch":
        url = str(payload.get("url") or "").strip()
        if not url:
            raise HTTPException(status_code=400, detail="`url` required")
        max_bytes = int(payload.get("max_bytes") or 20000)
        max_bytes = max(1, min(max_bytes, 250_000))
        timeout_sec = float(payload.get("timeout_sec") or 20)
        timeout_sec = max(1.0, min(timeout_sec, 120.0))
        headers = payload.get("headers") or {}
        if not isinstance(headers, dict):
            headers = {}

        async with httpx.AsyncClient(follow_redirects=True, timeout=timeout_sec) as client:
            r = await client.get(url, headers={str(k): str(v) for k, v in headers.items()})
            body = r.content[:max_bytes]
            text = body.decode("utf-8", errors="replace")
            return {
                "result": {
                    "url": str(r.url),
                    "status_code": r.status_code,
                    "content_type": r.headers.get("content-type"),
                    "text": text,
                    "truncated": len(r.content) > len(body),
                }
            }

    raise HTTPException(status_code=404, detail="Unknown MCP tool")

@app.get("/v1/studio/models", tags=["OpenAI Compatible"])
async def list_studio_models():
    """List available models from the models directory."""
    from pathlib import Path
    
    models = []
    models_dir = Path(settings.models_dir)
    
    if models_dir.exists():
        for item in models_dir.iterdir():
            if item.is_dir() and not item.name.startswith("."):
                # Skip non-model directories
                if any(item.name.startswith(x) for x in ["FLUX", "playground", "Hunyuan", "whisper", "Vibe"]):
                    continue
                    
                # Get size
                size_bytes = 0
                try:
                    for f in item.rglob("*.safetensors"):
                        size_bytes += f.stat().st_size
                    for f in item.rglob("*.bin"):
                        size_bytes += f.stat().st_size
                except:
                    pass
                
                # Get modified time
                try:
                    modified_at = item.stat().st_mtime
                except:
                    modified_at = None
                
                models.append({
                    "path": str(item),
                    "name": item.name,
                    "size_bytes": size_bytes if size_bytes > 0 else None,
                    "modified_at": modified_at,
                })
    
    return {"models": models}


# --- OpenAI Chat Completions Proxy ---

def _find_recipe_by_model(store: RecipeStore, model_name: str) -> Optional[Recipe]:
    """Find a recipe by served_model_name or id."""
    if not model_name:
        return None
    for recipe in store.list():
        if recipe.served_model_name == model_name or recipe.id == model_name:
            return recipe
    return None


async def _ensure_model_running(requested_model: str, store: RecipeStore) -> Optional[str]:
    """Ensure the requested model is running, auto-switching if needed.

    Returns None if model is ready, or an error message if switch failed.
    """
    import time
    import psutil
    from .process import launch_model

    if not requested_model:
        return None

    # Check what's currently running
    current = find_inference_process(settings.inference_port)

    # If the requested model is already running, we're done
    if current and current.served_model_name == requested_model:
        return None

    # Find recipe for the requested model
    recipe = _find_recipe_by_model(store, requested_model)
    if not recipe:
        # No recipe found - let LiteLLM handle it (might be an external model)
        return None

    # Need to switch models - acquire lock and switch
    async with _switch_lock:
        # Double-check after acquiring lock
        current = find_inference_process(settings.inference_port)
        if current and current.served_model_name == requested_model:
            return None

        logger.info(f"Auto-switching model: {current.served_model_name if current else 'none'} -> {requested_model}")

        # Evict current model
        await evict_model(force=False)
        await asyncio.sleep(2)

        # Launch new model
        success, pid, message = await launch_model(recipe)
        if not success:
            logger.error(f"Auto-switch failed to launch {requested_model}: {message}")
            return f"Failed to launch model {requested_model}: {message}"

        # Wait for readiness (up to 5 minutes)
        start = time.time()
        timeout = 300
        ready = False

        while time.time() - start < timeout:
            # Check if process crashed
            if pid and not psutil.pid_exists(pid):
                log_file = Path(f"/tmp/vllm_{recipe.id}.log")
                error_tail = ""
                if log_file.exists():
                    try:
                        error_tail = log_file.read_text()[-500:]
                    except Exception:
                        pass
                return f"Model {requested_model} crashed during startup: {error_tail[-200:]}"

            try:
                async with httpx.AsyncClient(timeout=5) as client:
                    r = await client.get(f"http://localhost:{settings.inference_port}/health")
                    if r.status_code == 200:
                        ready = True
                        break
            except Exception:
                pass

            await asyncio.sleep(3)

        if not ready:
            return f"Model {requested_model} failed to become ready (timeout)"

        logger.info(f"Auto-switch complete: {requested_model} is ready")
        return None


@app.post("/v1/chat/completions", tags=["OpenAI Compatible"])
async def chat_completions_proxy(request: Request, store: RecipeStore = Depends(get_store)):
    """Proxy chat completions to LiteLLM backend with auto-eviction support.

    If the requested model differs from the currently running model and a matching
    recipe exists, the controller will automatically evict the current model and
    launch the requested one before forwarding the request.
    """
    import os
    try:
        body = await request.body()

        # Parse request to get model and streaming flag
        try:
            data = json.loads(body)
            requested_model = data.get("model")
            is_streaming = data.get("stream", False)
        except Exception:
            requested_model = None
            is_streaming = False

        # Auto-switch model if needed
        if requested_model:
            switch_error = await _ensure_model_running(requested_model, store)
            if switch_error:
                raise HTTPException(status_code=503, detail=switch_error)

        # Use LiteLLM master key for backend auth (controller already validated the request)
        litellm_key = os.environ.get("LITELLM_MASTER_KEY", "sk-master")
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {litellm_key}",
        }

        litellm_url = "http://localhost:4100/v1/chat/completions"

        if is_streaming:
            async def stream_response():
                async with httpx.AsyncClient(timeout=300) as client:
                    async with client.stream("POST", litellm_url, content=body, headers=headers) as response:
                        async for chunk in response.aiter_bytes():
                            yield chunk

            return StreamingResponse(
                stream_response(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                }
            )
        else:
            async with httpx.AsyncClient(timeout=300) as client:
                response = await client.post(litellm_url, content=body, headers=headers)
                return JSONResponse(
                    content=response.json(),
                    status_code=response.status_code
                )
    except HTTPException:
        raise
    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="LiteLLM backend unavailable")
    except Exception as e:
        logger.error(f"Chat completions proxy error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
