# LiteLLM Proxy Configuration
# Handles API routing, format translation, and cost tracking

model_list:
  # GLM-4.7 - reasoning support with glm45 parser
  - model_name: "glm-4.7"
    litellm_params:
      model: "openai/glm-4.7"
      api_base: "http://host.docker.internal:8000/v1"
      api_key: "${INFERENCE_API_KEY:-sk-placeholder}"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_vision: false
      supports_reasoning: true

  # GLM-4.7-REAP-50 - 4xTP 2xPP, 200K context, FP8 KV, reasoning support
  - model_name: "GLM-4.7-REAP-50"
    litellm_params:
      model: "openai/GLM-4.7-REAP-50"
      api_base: "http://host.docker.internal:8000/v1"
      api_key: "${INFERENCE_API_KEY:-sk-placeholder}"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_vision: false
      supports_reasoning: true

  # MiniMax-M2.1
  - model_name: "MiniMax-M2.1"
    litellm_params:
      model: "openai/minimax-m2.1"
      api_base: "http://host.docker.internal:8000/v1"
      api_key: "${INFERENCE_API_KEY:-sk-placeholder}"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_vision: true
      supports_response_schema: true
      supports_reasoning: true

  - model_name: "MiniMax-M2.1-REAP-50"
    litellm_params:
      model: "openai/minimax-m2.1-reap-50"
      api_base: "http://host.docker.internal:8000/v1"
      api_key: "${INFERENCE_API_KEY:-sk-placeholder}"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_vision: true
      supports_response_schema: true
      supports_reasoning: true

  # MiroThinker-v1.5-235B-AWQ-4bit - Based on Qwen3-235B-A22B-Thinking-2507
  # Uses deepseek_r1 reasoning parser (not qwen3) and MCP-style tool calls
  - model_name: "MiroThinker-v1.5-235B-AWQ-4bit"
    litellm_params:
      model: "openai/MiroThinker-v1.5-235B-AWQ-4bit"
      api_base: "http://host.docker.internal:8000/v1"
      api_key: "${INFERENCE_API_KEY:-sk-placeholder}"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_vision: false
      supports_reasoning: true
      # Inference parameters - lower temperature for more deterministic tool calling
      # Official recommendation is 1.0, but 0.6 reduces tool call corruption
      temperature: 0.6
      top_p: 0.95
      max_tokens: 16384
      extra_body:
        repetition_penalty: 1.05

  # Wildcard catch-all for any model name -> route to local inference server
  - model_name: "*"
    litellm_params:
      model: "openai/*"
      api_base: "http://host.docker.internal:8000/v1"
      api_key: "${INFERENCE_API_KEY:-sk-placeholder}"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_vision: true
      supports_response_schema: true

# Router settings
router_settings:
  routing_strategy: "simple-shuffle"
  num_retries: 3
  timeout: 600
  retry_after: 3
  enable_pre_call_checks: false
  cooldown_time: 0
  allowed_fails: 3

# LiteLLM settings
litellm_settings:
  callbacks: tool_call_handler.proxy_handler_instance
  drop_params: false
  set_verbose: false
  request_timeout: 600
  telemetry: false
  stream_chunk_size: 1024
  num_retries: 3
  max_budget: 0
  budget_duration: 0
  modify_params: false
  enable_message_redaction: false
  force_ipv4: true

  # Redis caching for LLM responses (optional)
  cache: true
  cache_params:
    type: "redis"
    host: "vllm-studio-redis"
    port: 6379
    ttl: 3600
    namespace: "litellm:cache"
    supported_call_types: ["acompletion", "completion", "embedding"]

  # Prometheus metrics collection
  success_callback: ["prometheus"]
  failure_callback: ["prometheus"]

# General settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: "postgresql://postgres:postgres@vllm-studio-postgres:5432/litellm"
  ui_access_mode: "admin_only"
  json_logs: true
  store_model_in_db: true
  background_health_checks: false  # Disable - single model backend
  health_check_interval: null  # No health checks
